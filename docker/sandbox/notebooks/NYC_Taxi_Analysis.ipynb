{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b916896-bd4f-4593-87be-fcd8342e3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"NYC_Taxi_Analysis\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f0226a-8e53-46ae-80cb-b5662844b257",
   "metadata": {},
   "source": [
    "##  ***2022 Schema***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb91867-dc1b-4039-98fd-9077d5674490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2022 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39899180-baf8-4608-b929-22bc775ed682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"number of records of 2023 data is : {df_2022.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b975d8f-f74a-4c4d-977e-5ecab2e80a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d83fb-6d68-4b53-afc8-31949c61b561",
   "metadata": {},
   "source": [
    "##  ***2023 Schema***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4882ca-acff-40d2-9704-7744903fd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2023 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3096945-969b-4136-be73-b7c9f55d1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"number of records of 2023 data is : {df_2023.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6445195a-fc7f-4048-9537-20ed31528f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2023.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04633a7f-26f4-403f-88eb-653d2cce5442",
   "metadata": {},
   "source": [
    "## ***2024 Schema***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e9889b8-c958-4cf5-bacf-0c72824b9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2024 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bc8880-80a9-4e45-aa54-c1adddf2f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Number of records of 2024 data is : {df_2024.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f445c1ec-9c56-4714-bb5d-52d9bca62f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2024.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0512e18-d9f1-48e8-a47b-de534d8c3460",
   "metadata": {},
   "source": [
    "##  ***2025 Schema***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939f84bd-fe6d-4ec0-be04-5bbfa334d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2025 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94b7251-1d8f-4778-a729-b785e0a51747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Number of records of 2025 data is : {df_2025.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc8bf6ff-4719-414a-822a-1438db19b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2025.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed0e7b26-764c-4b54-b121-ef43a7b67750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(df_2022.schema==df_2023.schema == df_2024.schema == df_2025.schema)  \n",
    "\n",
    "#the result is False , so there is a schema mismatch , and to resolve this we should use mergeschema in spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b575eb6f-63ae-4dc6-aa23-f13f2892fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.option(\"mergeSchema\", \"true\").parquet(\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\",\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\",\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\"\n",
    "#)\n",
    "\n",
    "#the mergeschema in spark can not work , because it can not merge data types \"Failed to merge incompatible data types \"BIGINT\" and \"INT\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462ba7a-9cd1-405c-823b-d5dfc51b043c",
   "metadata": {},
   "source": [
    "# **Standard Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1cdc0-d279-4c79-8b21-a2f3022f86b5",
   "metadata": {},
   "source": [
    "### ⚠️ Observed Schema Mismatches\n",
    "\n",
    "- **Data type inconsistencies across years**\n",
    "  - `VendorID`: `long` (2022-2023) vs `integer` (2024–2025)\n",
    "  - `passenger_count`: `double` (2022-2023) vs `long` (2024–2025)\n",
    "  - `RatecodeID`: `double` (2022-2023) vs `long` (2024–2025)\n",
    "  - `PULocationID`, `DOLocationID`: `long` (2022-2023) vs `integer` (2024–2025)\n",
    "\n",
    "- **Column name variation**\n",
    "  - `airport_fee` (2022-2023) vs `Airport_fee` (2024–2025)\n",
    "\n",
    "- **Schema evolution**\n",
    "  - `cbd_congestion_fee` present only in 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d3ed60d-764c-4011-a1f0-68a4c8f6e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Standard Schema For all Years\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, DoubleType, StringType, TimestampType ,TimestampNTZType\n",
    "\n",
    "standard_schema = StructType([\n",
    "    StructField('VendorID', IntegerType(), True),\n",
    "    StructField('tpep_pickup_datetime', TimestampNTZType(), True),\n",
    "    StructField('tpep_dropoff_datetime', TimestampNTZType(), True),\n",
    "    StructField('passenger_count', LongType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', IntegerType(), True),\n",
    "    StructField('DOLocationID', IntegerType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('Airport_fee', DoubleType(), True), \n",
    "    StructField('cbd_congestion_fee', DoubleType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634a6c8-db36-4842-8d43-c5a9b15a5b9e",
   "metadata": {},
   "source": [
    "## Enforce Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69ab8520-b9c7-41dc-9f46-cb3413747c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an enforce function for compitability\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "def enforce_schema(df, schema):\n",
    "    # Rename existing lower-case column if needed\n",
    "    if \"airport_fee\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"airport_fee\", \"Airport_fee\")\n",
    "    \n",
    "    # Add missing columns and cast\n",
    "    for field in schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df.select([field.name for field in schema.fields])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96801c2c-4dbc-4b2e-a86f-29ce03309d80",
   "metadata": {},
   "source": [
    "### Read 2022 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c4773c3-0353-4c66-aa06-1a243bc3162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet files of same year\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2022\"\n",
    "\n",
    "dfs_2022_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2022-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2022_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f623eba-1784-4cfe-a8a3-2a85c2003bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2022_std[month] for month in sorted(dfs_2022_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2022_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf49bf5-069e-4578-baac-00307e4237ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2022_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e6a98-0d08-46c9-8fc7-85facdcf8bbb",
   "metadata": {},
   "source": [
    "### Read 2023 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03b95006-1561-4506-a05d-2160baded09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\"\n",
    "\n",
    "dfs_2023_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2023-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2023_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd061bb2-43f2-490e-b656-12f90b77de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2023_std[month] for month in sorted(dfs_2023_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2023_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e81cb057-0ed8-4826-b296-4651ba9e0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2023_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be04d12-c956-49ef-8835-f945ebaaffbf",
   "metadata": {},
   "source": [
    "### Read 2024 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "906ac1cb-4fd7-4ac8-b435-65f7c038eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\"\n",
    "\n",
    "dfs_2024_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2024-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2024_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1f13955-6166-4dc5-a536-ef9df3f3481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2024_std[month] for month in sorted(dfs_2024_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2024_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8603b87-d84e-4cc9-b865-66685f1582fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2024_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c4035-243f-4e1e-b640-808e19b9f289",
   "metadata": {},
   "source": [
    "### Read 2025 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1cfbf8f-79b1-4afa-acce-a18b7244675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\"\n",
    "\n",
    "dfs_2025_std = {}\n",
    "\n",
    "for m in range(1, 11):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2025-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2025_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b645ed93-b67f-4303-88ea-a1508fe9541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2025_std[month] for month in sorted(dfs_2025_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2025_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d37ac9a1-aab2-44f1-8385-de1c8dddaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2025_std.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33753981-7b2e-40d8-8d96-cda3a1710afd",
   "metadata": {},
   "source": [
    "# **Union All df Years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d60b9be8-4b0a-4855-acac-569e9e065cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check schema match\n",
    "print(df_2022_std.schema==df_2023_std.schema==df_2024_std.schema==df_2025_std.schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6392dc1e-b200-4863-bec6-145f513db4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    \n",
    "     df_2024_std\n",
    "    .unionByName(df_2025_std)\n",
    "    .unionByName(df_2023_std)\n",
    "    .unionByName(df_2022_std)\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5ac66e4-55b2-49d6-bdf7-6106c379d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"number of records of full df is : {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b752e77-2876-448e-bc67-f7ff9135825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0431a458-2860-4b4c-8ec5-be8af65ee803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5da9e4-225f-42fc-8a61-fdf991f4c36c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ba98a-24b4-4fa2-adf9-bcf71015502d",
   "metadata": {},
   "source": [
    "### **Rename Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57a28c29-4c9f-4575-91ea-b2d6ed6a9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_col_renamed = (\n",
    "    df\n",
    "    .withColumnRenamed(\"VendorID\", \"Vendor_ID\")\n",
    "    .withColumnRenamed(\"RatecodeID\", \"Ratecode_ID\")\n",
    "    .withColumnRenamed(\"PULocationID\", \"Pickup_Location_ID\")\n",
    "    .withColumnRenamed(\"DOLocationID\", \"Dropoff_Location_ID\")\n",
    "    .withColumnRenamed(\"extra\", \"extra_charges\")\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\",\"Trip_Pickup_DateTime\")\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\",\"Trip_Dropoff_DateTime\")\n",
    "    .withColumn(\n",
    "        'trip_duration_min',\n",
    "        f.round(\n",
    "            (f.unix_timestamp('Trip_Dropoff_DateTime') - f.unix_timestamp('Trip_Pickup_DateTime')) / 60,\n",
    "            2\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "859a5137-2cae-4aa6-b577-37e90a6be40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_col_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3031db-7124-464c-ad6a-ee619dd9453e",
   "metadata": {},
   "source": [
    "### **Dealing With Nulls**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541bbd8-8199-46bc-be69-dc637741e5c0",
   "metadata": {},
   "source": [
    "#### Handling NULL Values\n",
    "\n",
    "The dataset is cleaned by filling NULL values in specific columns with appropriate defaults:\n",
    "\n",
    "1. **Vendor_ID:** Replace NULLs with `99`.\n",
    "\n",
    "2. **Ratecode_ID:** Replace NULLs with `99`.\n",
    "\n",
    "3. **store_and_fwd_flag:** Replace NULLs with `'Unknown'`.\n",
    "\n",
    "4. **fare_amount:** Replace NULLs with `0.0`.\n",
    "\n",
    "5. **extra_charges:** Replace NULLs with `0.0`.\n",
    "\n",
    "6. **mta_tax:** Replace NULLs with `0.0`.\n",
    "\n",
    "7. **tip_amount:** Replace NULLs with the **mean value of the column** (`mean_tip`).\n",
    "\n",
    "8. **cbd_congestion_fee:** Replace NULLs with `0.0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7a70b76-3c95-4a78-b906-6891fd331288",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tip = df_col_renamed.select(f.mean(\"tip_amount\")).collect()[0][0]\n",
    "\n",
    "df_no_nulls = df_col_renamed.fillna({\n",
    "    'Vendor_ID': 99,\n",
    "    'Ratecode_ID': 99,\n",
    "    'store_and_fwd_flag':'Unknown',\n",
    "    'fare_amount': 0.0,\n",
    "    'extra_charges': 0.0,\n",
    "    'mta_tax': 0.0,\n",
    "    'tip_amount': mean_tip,\n",
    "    'cbd_congestion_fee': 0.0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86e0f9e3-08db-4501-b75c-cba4c38aec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_nulls.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad2d32-e66c-4d68-8fbe-d5acd9fd0695",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Filter bad Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb9fbc-2cd7-443e-90f6-9637f242f147",
   "metadata": {},
   "source": [
    "#### Filter Conditions\n",
    "\n",
    "The dataset is filtered based on the following conditions:\n",
    "\n",
    "1. **Passenger count:** Keep trips where the number of passengers is between **1 and 6** (inclusive).\n",
    "\n",
    "2. **Trip distance:** Keep trips where the distance is between **0 .1and 200**.\n",
    "\n",
    "3. **Pickup and dropoff locations:** Exclude trips where the pickup and dropoff locations are the same.\n",
    "\n",
    "4. **Trip duration:** Only keep trips with duration between **0.01 minutes and 120.1 minutes** (i.e., less than 2 hours).\n",
    "\n",
    "5. **Non-negative fees and amounts:** Exclude rows where any of the following columns are negative:\n",
    "   - `cbd_congestion_fee`\n",
    "   - `Airport_fee`\n",
    "   - `total_amount`\n",
    "   - `tip_amount`\n",
    "   - `fare_amount`\n",
    "   - `extra_charges`\n",
    "   - `improvement_surcharge`\n",
    "   - `mta_tax`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6543c81-f91d-4249-86b3-e3ceff10eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_no_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94bbe1a6-f156-477b-816f-a6ff16377084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6ab6b45-2195-432e-a1c1-b329444e6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_filtered = df_no_nulls.filter(\n",
    "    (f.col(\"passenger_count\").between(1, 6)) &\n",
    "    (f.col(\"trip_distance\").between(0.1,200)) &\n",
    "    (f.col(\"Pickup_Location_ID\") != f.col(\"Dropoff_Location_ID\")) &\n",
    "    (f.col(\"trip_duration_min\").between(0.01,120.1))&\n",
    "    (f.col(\"cbd_congestion_fee\")>=0.0)&\n",
    "    (f.col(\"Airport_fee\")>=0.0)&\n",
    "    (f.col(\"total_amount\")>=0.0)&\n",
    "    (f.col(\"tip_amount\")>=0.0)&\n",
    "    (f.col(\"fare_amount\")>=0.0)&\n",
    "    (f.col(\"extra_charges\")>=0.0)&\n",
    "    (f.col(\"improvement_surcharge\")>=0.0)&\n",
    "    (f.col(\"mta_tax\")>=0.0)\n",
    "    \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2dff0929-98a3-4d43-8f1c-a99041ab22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check applying of filter condition\n",
    "#from pyspark.sql import functions as f\n",
    "\n",
    "#df_filtered.where ( (f.col(\"passenger_count\") == 0) | (f.col(\"trip_distance\")>200) |(f.col(\"trip_duration_min\") >120.1) ).show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7505fd31-c48e-43b5-bfb7-553270f5bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220ce16-ba2f-443c-85d7-cfe8d925da65",
   "metadata": {},
   "source": [
    "### Creating new columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97254baa-e06b-4f47-ae8f-f3397c8613bb",
   "metadata": {},
   "source": [
    "#### Columns From Existing Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f04d6-55c4-4ff7-8d88-07bcc552ff08",
   "metadata": {},
   "source": [
    "#### Enriching Trip Data with Descriptive Columns\n",
    "\n",
    "This code adds and transforms several columns in the trip DataFrame:  \n",
    "\n",
    "- **Vendor_Name** – human-readable vendor names based on `Vendor_ID`.  \n",
    "- **Ratecode_Description** – descriptive text for `Ratecode_ID`.  \n",
    "- **Payment_Method** – descriptive text for `payment_type`.  \n",
    "- **Trip_Distance_Km** – converts `trip_distance` from miles to kilometers.  \n",
    "- **Year** – extracts the pickup year from `Trip_Pickup_DateTime`.  \n",
    "- **Month** – extracts the pickup month (abbreviated) from `Trip_Pickup_DateTime`.  \n",
    "- Renames `trip_distance` to `trip_distance_miles`.  \n",
    "\n",
    "All mappings use conditional logic (`when/otherwise`) to handle known values and assign `\"Unknown\"` for others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62ed7907-7243-4c0b-822a-1a2e6daa714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_derived_col = (\n",
    "    df_filtered\n",
    "    .withColumn(\n",
    "        \"Vendor_Name\",\n",
    "        f.when(f.col(\"Vendor_ID\") == 1, f.lit(\"Creative Mobile Technologies, LLC\"))\n",
    "         .when(f.col(\"Vendor_ID\") == 2, f.lit(\"Curb Mobility, LLC\"))\n",
    "         .when(f.col(\"Vendor_ID\") == 6, f.lit(\"Myle Technologies Inc\"))\n",
    "         .when(f.col(\"Vendor_ID\") == 7, f.lit(\"Helix\"))\n",
    "         .otherwise(f.lit(\"Unknown\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Ratecode_Description\",\n",
    "        f.when(f.col(\"Ratecode_ID\") == 1, f.lit(\"Standard rate\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 2, f.lit(\"JFK\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 3, f.lit(\"Newark\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 4, f.lit(\"Nassau or Westchester\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 5, f.lit(\"Negotiated fare\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 6, f.lit(\"Group ride\"))\n",
    "         .otherwise(f.lit(\"Unknown\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Payment_Method\",\n",
    "        f.when(f.col(\"payment_type\") == 0, f.lit(\"Flex Fare trip\"))\n",
    "         .when(f.col(\"payment_type\") == 1, f.lit(\"Credit Card\"))\n",
    "         .when(f.col(\"payment_type\") == 2, f.lit(\"Cash\"))\n",
    "         .when(f.col(\"payment_type\") == 3, f.lit(\"No Charge\"))\n",
    "         .when(f.col(\"payment_type\") == 4, f.lit(\"Dispute\"))\n",
    "         .when(f.col(\"payment_type\") == 5, f.lit(\"Unknown\"))\n",
    "         .when(f.col(\"payment_type\") == 6, f.lit(\"Voided trip\"))\n",
    "         .otherwise(f.lit(\"Unknown\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Trip_Distance_Km\",\n",
    "        f.round(f.col(\"trip_distance\") * 1.609 ,2)\n",
    "    )\n",
    "\n",
    "    .withColumn(\n",
    "        \"Year\", \n",
    "        f.year(f.col('Trip_Pickup_DateTime'))\n",
    "    )\n",
    "\n",
    "   .withColumn(\n",
    "        \"Month\",\n",
    "         f.date_format(f.col(\"Trip_Pickup_DateTime\"), \"MMM\")\n",
    ")\n",
    "    \n",
    "    \n",
    "    .withColumnRenamed('trip_distance','trip_distance_miles')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51d22102-ce7e-44e3-9311-fcd6a88eb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_derived_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763c7b3-b787-47e1-adba-e410e1a59dad",
   "metadata": {},
   "source": [
    "#### Columns from lookup_zones_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a5099-a505-499e-b7a0-e0fc4a3461bd",
   "metadata": {},
   "source": [
    "#### Adding Zone Information\n",
    "\n",
    "This  adds new columns to the DataFrame by left-joining with `lookup_zones_df` on `Pickup_Location_ID` and `Dropoff_Location_ID`.  \n",
    "\n",
    "**Columns added:**  \n",
    "- `Pickup_Borough`, `Pickup_Zone`, `Pickup_Service_Zone`  \n",
    "- `Dropoff_Borough`, `Dropoff_Zone`, `Dropoff_Service_Zone`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fe61a91-f4a3-45c4-9b0a-c1b48cb31659",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_zones_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferschema\",\"True\")\n",
    "         .csv(\"/opt/spark/resources/taxi_zone_lookup.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc272964-c29f-4bdb-9f77-d89a55a55f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup_zones_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee2b752b-69ff-4cfa-bc33-f351b7883846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup_zones_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e73877d-6f85-4193-ae5d-4de5bfa3447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "pickup_zone = lookup_zones_df.alias('pickup')\n",
    "dropoff_zone = lookup_zones_df.alias('dropoff')\n",
    "\n",
    "df_joined = df_derived_col \\\n",
    "    .join(\n",
    "        pickup_zone,\n",
    "        df_derived_col['Pickup_Location_ID'] == pickup_zone['LocationID'],\n",
    "        how='left'\n",
    "    ) \\\n",
    "    .join(\n",
    "        dropoff_zone,\n",
    "        df_derived_col['Dropoff_Location_ID'] == dropoff_zone['LocationID'],\n",
    "        how='left'\n",
    "    ) \\\n",
    "    .select(\n",
    "        df_derived_col['*'],  # all original columns\n",
    "        f.col('pickup.Borough').alias('Pickup_Borough'),\n",
    "        f.col('pickup.Zone').alias('Pickup_Zone'),\n",
    "        f.col('pickup.service_zone').alias('Pickup_Service_Zone'),\n",
    "        f.col('dropoff.Borough').alias('Dropoff_Borough'),\n",
    "        f.col('dropoff.Zone').alias('Dropoff_Zone'),\n",
    "        f.col('dropoff.service_zone').alias('Dropoff_Service_Zone')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13672c57-04ab-4fa1-9407-eaeae558a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_joined.count()\n",
    "#df_joined.show()\n",
    "#df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b664f0-1dc1-4d04-b2ca-8cad4a59e32f",
   "metadata": {},
   "source": [
    "# **Final df**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "994da499-a93a-4233-80f7-61242a41efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_joined.select(\n",
    "    'Vendor_ID',\n",
    "    'Vendor_Name',\n",
    "    'Trip_Pickup_DateTime',\n",
    "    'Trip_Dropoff_DateTime',\n",
    "    'passenger_count',\n",
    "    'Pickup_Location_ID',\n",
    "    'Pickup_Borough',\n",
    "    'Pickup_Zone',\n",
    "    'Pickup_Service_Zone',\n",
    "    'Dropoff_Location_ID',\n",
    "    'Dropoff_Borough',\n",
    "    'Dropoff_Zone',\n",
    "    'Dropoff_Service_Zone',\n",
    "    'Trip_Distance_Km',\n",
    "    'trip_distance_miles',\n",
    "    'Ratecode_ID',\n",
    "    'Ratecode_Description',\n",
    "    'payment_type',\n",
    "    'Payment_Method',\n",
    "    'trip_duration_min',\n",
    "    'fare_amount',\n",
    "    'extra_charges',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'Airport_fee',\n",
    "    'cbd_congestion_fee',\n",
    "    'Year',\n",
    "    'Month'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f0f29a6-2c2a-4d5d-86f7-7b9f9f1efb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40391ee7-0153-48b0-84c1-604fe9d8264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records of original df: 159372196\n",
      "Number of records after Cleaning Data: 130902515\n",
      "Number of filtered records: 28469681\n",
      "Percentage of bad data from whole data: 17.86%\n"
     ]
    }
   ],
   "source": [
    "# Store the original count\n",
    "original_count = df.count()\n",
    "\n",
    "# Store the filtered count\n",
    "final_count = final_df.count()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of records of original df: {original_count}\")\n",
    "print(f\"Number of records after Cleaning Data: {final_count}\")\n",
    "print(f\"Number of filtered records: {original_count - final_count}\")\n",
    "print(f\"Percentage of bad data from whole data: {(original_count - final_count) / original_count * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84f3cf02-1de0-47dc-bb40-8b82e5745fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips_Cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97e84511-ef3b-4536-b1ff-3cdc62a79416",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips_Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f77464a-2d71-4e8f-8177-4efda63f211c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130902515"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c3b67-b2bc-4de7-b615-d67a40354fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
