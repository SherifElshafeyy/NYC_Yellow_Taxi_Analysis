{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b916896-bd4f-4593-87be-fcd8342e3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"NYC_Taxi_Analysis\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7655cb4c-f111-417a-8071-cddd5adb32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f0226a-8e53-46ae-80cb-b5662844b257",
   "metadata": {},
   "source": [
    "##  ***2022 Schema***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fb91867-dc1b-4039-98fd-9077d5674490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2022 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39899180-baf8-4608-b929-22bc775ed682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"number of records of 2023 data is : {df_2022.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b975d8f-f74a-4c4d-977e-5ecab2e80a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d83fb-6d68-4b53-afc8-31949c61b561",
   "metadata": {},
   "source": [
    "##  ***2023 Schema***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4882ca-acff-40d2-9704-7744903fd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2023 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3096945-969b-4136-be73-b7c9f55d1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"number of records of 2023 data is : {df_2023.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6445195a-fc7f-4048-9537-20ed31528f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2023.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04633a7f-26f4-403f-88eb-653d2cce5442",
   "metadata": {},
   "source": [
    "## ***2024 Schema***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e9889b8-c958-4cf5-bacf-0c72824b9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2024 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bc8880-80a9-4e45-aa54-c1adddf2f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Number of records of 2024 data is : {df_2024.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f445c1ec-9c56-4714-bb5d-52d9bca62f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2024.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0512e18-d9f1-48e8-a47b-de534d8c3460",
   "metadata": {},
   "source": [
    "##  ***2025 Schema***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939f84bd-fe6d-4ec0-be04-5bbfa334d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2025 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a94b7251-1d8f-4778-a729-b785e0a51747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Number of records of 2025 data is : {df_2025.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc8bf6ff-4719-414a-822a-1438db19b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2025.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed0e7b26-764c-4b54-b121-ef43a7b67750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(df_2022.schema==df_2023.schema == df_2024.schema == df_2025.schema)  \n",
    "\n",
    "#the result is False , so there is a schema mismatch , and to resolve this we should use mergeschema in spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b575eb6f-63ae-4dc6-aa23-f13f2892fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.option(\"mergeSchema\", \"true\").parquet(\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\",\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\",\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\"\n",
    "#)\n",
    "\n",
    "#the mergeschema in spark can not work , because it can not merge data types \"Failed to merge incompatible data types \"BIGINT\" and \"INT\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462ba7a-9cd1-405c-823b-d5dfc51b043c",
   "metadata": {},
   "source": [
    "# **Standard Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1cdc0-d279-4c79-8b21-a2f3022f86b5",
   "metadata": {},
   "source": [
    "### ⚠️ Observed Schema Mismatches\n",
    "\n",
    "- **Data type inconsistencies across years**\n",
    "  - `VendorID`: `long` (2022-2023) vs `integer` (2024–2025)\n",
    "  - `passenger_count`: `double` (2022-2023) vs `long` (2024–2025)\n",
    "  - `RatecodeID`: `double` (2022-2023) vs `long` (2024–2025)\n",
    "  - `PULocationID`, `DOLocationID`: `long` (2022-2023) vs `integer` (2024–2025)\n",
    "\n",
    "- **Column name variation**\n",
    "  - `airport_fee` (2022-2023) vs `Airport_fee` (2024–2025)\n",
    "\n",
    "- **Schema evolution**\n",
    "  - `cbd_congestion_fee` present only in 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf17dad-5d63-42c8-a2dd-9a2f1bb1df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for minio \n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1356157e-f4e5-46a4-906b-02a789de9d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3ed60d-764c-4011-a1f0-68a4c8f6e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Standard Schema For all Years\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, DoubleType, StringType, TimestampType ,TimestampNTZType\n",
    "\n",
    "standard_schema = StructType([\n",
    "    StructField('VendorID', IntegerType(), True),\n",
    "    StructField('tpep_pickup_datetime', TimestampNTZType(), True),\n",
    "    StructField('tpep_dropoff_datetime', TimestampNTZType(), True),\n",
    "    StructField('passenger_count', LongType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', IntegerType(), True),\n",
    "    StructField('DOLocationID', IntegerType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('Airport_fee', DoubleType(), True), \n",
    "    StructField('cbd_congestion_fee', DoubleType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634a6c8-db36-4842-8d43-c5a9b15a5b9e",
   "metadata": {},
   "source": [
    "## Enforce Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ab8520-b9c7-41dc-9f46-cb3413747c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an enforce function for compitability\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "def enforce_schema(df, schema):\n",
    "    # Rename existing lower-case column if needed\n",
    "    if \"airport_fee\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"airport_fee\", \"Airport_fee\")\n",
    "    \n",
    "    # Add missing columns and cast\n",
    "    for field in schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df.select([field.name for field in schema.fields])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96801c2c-4dbc-4b2e-a86f-29ce03309d80",
   "metadata": {},
   "source": [
    "### Read 2022 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4773c3-0353-4c66-aa06-1a243bc3162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet files of same year\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2022\"\n",
    "\n",
    "dfs_2022_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2022-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2022_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f623eba-1784-4cfe-a8a3-2a85c2003bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2022_std[month] for month in sorted(dfs_2022_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2022_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cf49bf5-069e-4578-baac-00307e4237ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39656098"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_2022_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e6a98-0d08-46c9-8fc7-85facdcf8bbb",
   "metadata": {},
   "source": [
    "### Read 2023 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03b95006-1561-4506-a05d-2160baded09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\"\n",
    "\n",
    "dfs_2023_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2023-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2023_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd061bb2-43f2-490e-b656-12f90b77de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2023_std[month] for month in sorted(dfs_2023_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2023_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e81cb057-0ed8-4826-b296-4651ba9e0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2023_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be04d12-c956-49ef-8835-f945ebaaffbf",
   "metadata": {},
   "source": [
    "### Read 2024 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "906ac1cb-4fd7-4ac8-b435-65f7c038eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\"\n",
    "\n",
    "dfs_2024_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2024-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2024_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f13955-6166-4dc5-a536-ef9df3f3481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2024_std[month] for month in sorted(dfs_2024_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2024_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8603b87-d84e-4cc9-b865-66685f1582fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2024_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c4035-243f-4e1e-b640-808e19b9f289",
   "metadata": {},
   "source": [
    "### Read 2025 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1cfbf8f-79b1-4afa-acce-a18b7244675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read each parquet file of specified year individuall and then union all parqyet\n",
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\"\n",
    "\n",
    "dfs_2025_std = {}\n",
    "\n",
    "for m in range(1, 11):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2025-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2025_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b645ed93-b67f-4303-88ea-a1508fe9541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2025_std[month] for month in sorted(dfs_2025_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2025_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d37ac9a1-aab2-44f1-8385-de1c8dddaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2025_std.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33753981-7b2e-40d8-8d96-cda3a1710afd",
   "metadata": {},
   "source": [
    "# **Union All df Years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d60b9be8-4b0a-4855-acac-569e9e065cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check schema match\n",
    "print(df_2022_std.schema==df_2023_std.schema==df_2024_std.schema==df_2025_std.schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6392dc1e-b200-4863-bec6-145f513db4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    \n",
    "     df_2024_std\n",
    "    .unionByName(df_2025_std)\n",
    "    .unionByName(df_2023_std)\n",
    "    .unionByName(df_2022_std)\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1883d21-d612-4a73-8296-7ac940fe2579",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o10552.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://staging-output/taxi/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o10552.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:792)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\") \\\n",
    "  .parquet(\"s3a://staging-output/taxi/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5ac66e4-55b2-49d6-bdf7-6106c379d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"number of records of full df is : {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b752e77-2876-448e-bc67-f7ff9135825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0431a458-2860-4b4c-8ec5-be8af65ee803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5da9e4-225f-42fc-8a61-fdf991f4c36c",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ba98a-24b4-4fa2-adf9-bcf71015502d",
   "metadata": {},
   "source": [
    "### **Rename Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57a28c29-4c9f-4575-91ea-b2d6ed6a9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_col_renamed = (\n",
    "    df\n",
    "    .withColumnRenamed(\"VendorID\", \"Vendor_ID\")\n",
    "    .withColumnRenamed(\"RatecodeID\", \"Ratecode_ID\")\n",
    "    .withColumnRenamed(\"PULocationID\", \"Pickup_Location_ID\")\n",
    "    .withColumnRenamed(\"DOLocationID\", \"Dropoff_Location_ID\")\n",
    "    .withColumnRenamed(\"extra\", \"extra_charges\")\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\",\"Trip_Pickup_DateTime\")\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\",\"Trip_Dropoff_DateTime\")\n",
    "    .withColumn(\n",
    "        'trip_duration_min',\n",
    "        f.round(\n",
    "            (f.unix_timestamp('Trip_Dropoff_DateTime') - f.unix_timestamp('Trip_Pickup_DateTime')) / 60,\n",
    "            2\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "859a5137-2cae-4aa6-b577-37e90a6be40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_col_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3031db-7124-464c-ad6a-ee619dd9453e",
   "metadata": {},
   "source": [
    "### **Dealing With Nulls**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541bbd8-8199-46bc-be69-dc637741e5c0",
   "metadata": {},
   "source": [
    "#### Handling NULL Values\n",
    "\n",
    "The dataset is cleaned by filling NULL values in specific columns with appropriate defaults:\n",
    "\n",
    "1. **Vendor_ID:** Replace NULLs with `99`.\n",
    "\n",
    "2. **Ratecode_ID:** Replace NULLs with `99`.\n",
    "\n",
    "3. **store_and_fwd_flag:** Replace NULLs with `'Unknown'`.\n",
    "\n",
    "4. **fare_amount:** Replace NULLs with `0.0`.\n",
    "\n",
    "5. **extra_charges:** Replace NULLs with `0.0`.\n",
    "\n",
    "6. **mta_tax:** Replace NULLs with `0.0`.\n",
    "\n",
    "7. **tip_amount:** Replace NULLs with the **mean value of the column** (`mean_tip`).\n",
    "\n",
    "8. **cbd_congestion_fee:** Replace NULLs with `0.0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7a70b76-3c95-4a78-b906-6891fd331288",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tip = df_col_renamed.select(f.mean(\"tip_amount\")).collect()[0][0]\n",
    "\n",
    "df_no_nulls = df_col_renamed.fillna({\n",
    "    'Vendor_ID': 99,\n",
    "    'Ratecode_ID': 99,\n",
    "    'store_and_fwd_flag':'Unknown',\n",
    "    'fare_amount': 0.0,\n",
    "    'extra_charges': 0.0,\n",
    "    'mta_tax': 0.0,\n",
    "    'tip_amount': mean_tip,\n",
    "    'cbd_congestion_fee': 0.0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86e0f9e3-08db-4501-b75c-cba4c38aec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_nulls.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad2d32-e66c-4d68-8fbe-d5acd9fd0695",
   "metadata": {},
   "source": [
    "### **Filter bad Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb9fbc-2cd7-443e-90f6-9637f242f147",
   "metadata": {},
   "source": [
    "#### Filter Conditions\n",
    "\n",
    "The dataset is filtered based on the following conditions:\n",
    "\n",
    "1. **Passenger count:** Keep trips where the number of passengers is between **1 and 6** (inclusive).\n",
    "\n",
    "2. **Trip distance:** Keep trips where the distance is between **0 .1and 200**.\n",
    "\n",
    "3. **Pickup and dropoff locations:** Exclude trips where the pickup and dropoff locations are the same.\n",
    "\n",
    "4. **Trip duration:** Only keep trips with duration between **0.01 minutes and 120.1 minutes** (i.e., less than 2 hours).\n",
    "\n",
    "5. **Non-negative fees and amounts:** Exclude rows where any of the following columns are negative:\n",
    "   - `cbd_congestion_fee`\n",
    "   - `Airport_fee`\n",
    "   - `total_amount`\n",
    "   - `tip_amount`\n",
    "   - `fare_amount`\n",
    "   - `extra_charges`\n",
    "   - `improvement_surcharge`\n",
    "   - `mta_tax`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6543c81-f91d-4249-86b3-e3ceff10eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_no_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94bbe1a6-f156-477b-816f-a6ff16377084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6ab6b45-2195-432e-a1c1-b329444e6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_filtered = df_no_nulls.filter(\n",
    "    (f.col(\"passenger_count\").between(1, 6)) &\n",
    "    (f.col(\"trip_distance\").between(0.1,200)) &\n",
    "    (f.col(\"Pickup_Location_ID\") != f.col(\"Dropoff_Location_ID\")) &\n",
    "    (f.col(\"trip_duration_min\").between(0.01,120.1))&\n",
    "    (f.col(\"cbd_congestion_fee\")>=0.0)&\n",
    "    (f.col(\"Airport_fee\")>=0.0)&\n",
    "    (f.col(\"total_amount\")>=0.0)&\n",
    "    (f.col(\"tip_amount\")>=0.0)&\n",
    "    (f.col(\"fare_amount\")>=0.0)&\n",
    "    (f.col(\"extra_charges\")>=0.0)&\n",
    "    (f.col(\"improvement_surcharge\")>=0.0)&\n",
    "    (f.col(\"mta_tax\")>=0.0)\n",
    "    \n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2dff0929-98a3-4d43-8f1c-a99041ab22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check applying of filter condition\n",
    "#from pyspark.sql import functions as f\n",
    "\n",
    "#df_filtered.where ( (f.col(\"passenger_count\") == 0) | (f.col(\"trip_distance\")>200) |(f.col(\"trip_duration_min\") >120.1) ).show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7505fd31-c48e-43b5-bfb7-553270f5bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2220ce16-ba2f-443c-85d7-cfe8d925da65",
   "metadata": {},
   "source": [
    "### **Creating new columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97254baa-e06b-4f47-ae8f-f3397c8613bb",
   "metadata": {},
   "source": [
    "#### - Columns From Existing Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f04d6-55c4-4ff7-8d88-07bcc552ff08",
   "metadata": {},
   "source": [
    "#### Enriching Trip Data with Descriptive Columns\n",
    "\n",
    "This code adds and transforms several columns in the trip DataFrame:  \n",
    "\n",
    "- **Vendor_Name** – human-readable vendor names based on `Vendor_ID`.  \n",
    "- **Ratecode_Description** – descriptive text for `Ratecode_ID`.  \n",
    "- **Payment_Method** – descriptive text for `payment_type`.  \n",
    "- **Trip_Distance_Km** – converts `trip_distance` from miles to kilometers.  \n",
    "- **Year** – extracts the pickup year from `Trip_Pickup_DateTime`.  \n",
    "- **Month** – extracts the pickup month (abbreviated) from `Trip_Pickup_DateTime`.  \n",
    "- Renames `trip_distance` to `trip_distance_miles`.  \n",
    "\n",
    "All mappings use conditional logic (`when/otherwise`) to handle known values and assign `\"Unknown\"` for others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62ed7907-7243-4c0b-822a-1a2e6daa714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_derived_col = (\n",
    "    df_filtered\n",
    "    .withColumn(\n",
    "        \"Vendor_Name\",\n",
    "        f.when(f.col(\"Vendor_ID\") == 1, f.lit(\"Creative Mobile Technologies, LLC\"))\n",
    "         .when(f.col(\"Vendor_ID\") == 2, f.lit(\"Curb Mobility, LLC\"))\n",
    "         .when(f.col(\"Vendor_ID\") == 6, f.lit(\"Myle Technologies Inc\"))\n",
    "         .when(f.col(\"Vendor_ID\") == 7, f.lit(\"Helix\"))\n",
    "         .otherwise(f.lit(\"Unknown\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Ratecode_Description\",\n",
    "        f.when(f.col(\"Ratecode_ID\") == 1, f.lit(\"Standard rate\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 2, f.lit(\"JFK\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 3, f.lit(\"Newark\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 4, f.lit(\"Nassau or Westchester\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 5, f.lit(\"Negotiated fare\"))\n",
    "         .when(f.col(\"Ratecode_ID\") == 6, f.lit(\"Group ride\"))\n",
    "         .otherwise(f.lit(\"Unknown\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Payment_Method\",\n",
    "        f.when(f.col(\"payment_type\") == 0, f.lit(\"Flex Fare trip\"))\n",
    "         .when(f.col(\"payment_type\") == 1, f.lit(\"Credit Card\"))\n",
    "         .when(f.col(\"payment_type\") == 2, f.lit(\"Cash\"))\n",
    "         .when(f.col(\"payment_type\") == 3, f.lit(\"No Charge\"))\n",
    "         .when(f.col(\"payment_type\") == 4, f.lit(\"Dispute\"))\n",
    "         .when(f.col(\"payment_type\") == 5, f.lit(\"Unknown\"))\n",
    "         .when(f.col(\"payment_type\") == 6, f.lit(\"Voided trip\"))\n",
    "         .otherwise(f.lit(\"Unknown\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Trip_Distance_Km\",\n",
    "        f.round(f.col(\"trip_distance\") * 1.609 ,2)\n",
    "    )\n",
    "\n",
    "    .withColumn(\n",
    "        \"Year\", \n",
    "        f.year(f.col('Trip_Pickup_DateTime'))\n",
    "    )\n",
    "\n",
    "   .withColumn(\n",
    "        \"Month\",\n",
    "         f.date_format(f.col(\"Trip_Pickup_DateTime\"), \"MMM\")\n",
    ")\n",
    "    \n",
    "    \n",
    "    .withColumnRenamed('trip_distance','trip_distance_miles')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "51d22102-ce7e-44e3-9311-fcd6a88eb88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_derived_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763c7b3-b787-47e1-adba-e410e1a59dad",
   "metadata": {},
   "source": [
    "#### Columns from lookup_zones_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a5099-a505-499e-b7a0-e0fc4a3461bd",
   "metadata": {},
   "source": [
    "#### Adding Zone Information\n",
    "\n",
    "This  adds new columns to the DataFrame by left-joining with `lookup_zones_df` on `Pickup_Location_ID` and `Dropoff_Location_ID`.  \n",
    "\n",
    "**Columns added:**  \n",
    "- `Pickup_Borough`, `Pickup_Zone`, `Pickup_Service_Zone`  \n",
    "- `Dropoff_Borough`, `Dropoff_Zone`, `Dropoff_Service_Zone`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fe61a91-f4a3-45c4-9b0a-c1b48cb31659",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_zones_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferschema\",\"True\")\n",
    "         .csv(\"/opt/spark/resources/taxi_zone_lookup.csv\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fc272964-c29f-4bdb-9f77-d89a55a55f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup_zones_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee2b752b-69ff-4cfa-bc33-f351b7883846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup_zones_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e73877d-6f85-4193-ae5d-4de5bfa3447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "pickup_zone = lookup_zones_df.alias('pickup')\n",
    "dropoff_zone = lookup_zones_df.alias('dropoff')\n",
    "\n",
    "df_joined = df_derived_col \\\n",
    "    .join(\n",
    "        pickup_zone,\n",
    "        df_derived_col['Pickup_Location_ID'] == pickup_zone['LocationID'],\n",
    "        how='left'\n",
    "    ) \\\n",
    "    .join(\n",
    "        dropoff_zone,\n",
    "        df_derived_col['Dropoff_Location_ID'] == dropoff_zone['LocationID'],\n",
    "        how='left'\n",
    "    ) \\\n",
    "    .select(\n",
    "        df_derived_col['*'],  # all original columns\n",
    "        f.col('pickup.Borough').alias('Pickup_Borough'),\n",
    "        f.col('pickup.Zone').alias('Pickup_Zone'),\n",
    "        f.col('pickup.service_zone').alias('Pickup_Service_Zone'),\n",
    "        f.col('dropoff.Borough').alias('Dropoff_Borough'),\n",
    "        f.col('dropoff.Zone').alias('Dropoff_Zone'),\n",
    "        f.col('dropoff.service_zone').alias('Dropoff_Service_Zone')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13672c57-04ab-4fa1-9407-eaeae558a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_joined.count()\n",
    "#df_joined.show()\n",
    "#df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b664f0-1dc1-4d04-b2ca-8cad4a59e32f",
   "metadata": {},
   "source": [
    "# **Final df**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "994da499-a93a-4233-80f7-61242a41efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_joined.select(\n",
    "    'Vendor_ID',\n",
    "    'Vendor_Name',\n",
    "    'Trip_Pickup_DateTime',\n",
    "    'Trip_Dropoff_DateTime',\n",
    "    'passenger_count',\n",
    "    'Pickup_Location_ID',\n",
    "    'Pickup_Borough',\n",
    "    'Pickup_Zone',\n",
    "    'Pickup_Service_Zone',\n",
    "    'Dropoff_Location_ID',\n",
    "    'Dropoff_Borough',\n",
    "    'Dropoff_Zone',\n",
    "    'Dropoff_Service_Zone',\n",
    "    'Trip_Distance_Km',\n",
    "    'trip_distance_miles',\n",
    "    'Ratecode_ID',\n",
    "    'Ratecode_Description',\n",
    "    'payment_type',\n",
    "    'Payment_Method',\n",
    "    'trip_duration_min',\n",
    "    'fare_amount',\n",
    "    'extra_charges',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'total_amount',\n",
    "    'congestion_surcharge',\n",
    "    'Airport_fee',\n",
    "    'cbd_congestion_fee',\n",
    "    'Year',\n",
    "    'Month'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40391ee7-0153-48b0-84c1-604fe9d8264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records of original df: 159372196\n",
      "Number of records after Cleaning Data: 130902515\n",
      "Number of filtered records: 28469681\n",
      "Percentage of bad data from whole data: 17.86%\n"
     ]
    }
   ],
   "source": [
    "# Store the original count\n",
    "original_count = df.count()\n",
    "\n",
    "# Store the filtered count\n",
    "final_count = final_df.count()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of records of original df: {original_count}\")\n",
    "print(f\"Number of records after Cleaning Data: {final_count}\")\n",
    "print(f\"Number of filtered records: {original_count - final_count}\")\n",
    "print(f\"Percentage of bad data from whole data: {(original_count - final_count) / original_count * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be266538-d543-4851-b95e-bee56c20a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84f3cf02-1de0-47dc-bb40-8b82e5745fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.write.mode(\"overwrite\").parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips_Cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97e84511-ef3b-4536-b1ff-3cdc62a79416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleaned=spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips_Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f77464a-2d71-4e8f-8177-4efda63f211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "610c3b67-b2bc-4de7-b615-d67a40354fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading year 2022...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=42>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/spark/python/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o10561.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_162/4282569352.py\", line 63, in main\n",
      "    df_2022_std = load_year_standardized(spark, 2022, 12)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_162/4282569352.py\", line 48, in load_year_standardized\n",
      "    df = spark.read.parquet(path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 544, in parquet\n",
      "    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o10802.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: An error occurred while calling o10802.parquet\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o10802.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m         spark\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading year 2022...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m     df_2022_std \u001b[38;5;241m=\u001b[39m \u001b[43mload_year_standardized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2022\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     count_2022 \u001b[38;5;241m=\u001b[39m df_2022_std\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2022 loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_2022\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m, in \u001b[0;36mload_year_standardized\u001b[0;34m(spark, year, max_month)\u001b[0m\n\u001b[1;32m     46\u001b[0m     month \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/yellow_tripdata_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     dfs_std[month] \u001b[38;5;241m=\u001b[39m enforce_schema(df, standard_schema)\n\u001b[1;32m     50\u001b[0m dfs_list \u001b[38;5;241m=\u001b[39m [dfs_std[month] \u001b[38;5;28;01mfor\u001b[39;00m month \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(dfs_std\u001b[38;5;241m.\u001b[39mkeys())]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o10802.parquet"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, LongType, DoubleType,\n",
    "    StringType, TimestampNTZType,\n",
    ")\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "standard_schema = StructType([\n",
    "    StructField('VendorID', IntegerType(), True),\n",
    "    StructField('tpep_pickup_datetime', TimestampNTZType(), True),\n",
    "    StructField('tpep_dropoff_datetime', TimestampNTZType(), True),\n",
    "    StructField('passenger_count', LongType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', IntegerType(), True),\n",
    "    StructField('DOLocationID', IntegerType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('Airport_fee', DoubleType(), True),\n",
    "    StructField('cbd_congestion_fee', DoubleType(), True),\n",
    "])\n",
    "\n",
    "def enforce_schema(df: DataFrame, schema: StructType) -> DataFrame:\n",
    "    if \"airport_fee\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"airport_fee\", \"Airport_fee\")\n",
    "    for field in schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "    return df.select([field.name for field in schema.fields])\n",
    "\n",
    "def load_year_standardized(spark: SparkSession, year: int, max_month: int) -> DataFrame:\n",
    "    base_path = f\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/{year}\"\n",
    "    dfs_std = {}\n",
    "    for m in range(1, max_month + 1):\n",
    "        month = f\"{m:02d}\"\n",
    "        path = f\"{base_path}/yellow_tripdata_{year}-{month}.parquet\"\n",
    "        df = spark.read.parquet(path)\n",
    "        dfs_std[month] = enforce_schema(df, standard_schema)\n",
    "    dfs_list = [dfs_std[month] for month in sorted(dfs_std.keys())]\n",
    "    return reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n",
    "\n",
    "def main():\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"NYC_Taxi_Union_All_Years\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(\"Loading year 2022...\")\n",
    "        df_2022_std = load_year_standardized(spark, 2022, 12)\n",
    "        count_2022 = df_2022_std.count()\n",
    "        print(f\"2022 loaded: {count_2022} records\")\n",
    "        \n",
    "        print(\"Loading year 2023...\")\n",
    "        df_2023_std = load_year_standardized(spark, 2023, 12)\n",
    "        count_2023 = df_2023_std.count()\n",
    "        print(f\"2023 loaded: {count_2023} records\")\n",
    "        \n",
    "        print(\"Loading year 2024...\")\n",
    "        df_2024_std = load_year_standardized(spark, 2024, 12)\n",
    "        count_2024 = df_2024_std.count()\n",
    "        print(f\"2024 loaded: {count_2024} records\")\n",
    "        \n",
    "        print(\"Loading year 2025...\")\n",
    "        df_2025_std = load_year_standardized(spark, 2025, 10)\n",
    "        count_2025 = df_2025_std.count()\n",
    "        print(f\"2025 loaded: {count_2025} records\")\n",
    "\n",
    "        # Schema check\n",
    "        schema_match = (df_2022_std.schema == df_2023_std.schema == \n",
    "                       df_2024_std.schema == df_2025_std.schema)\n",
    "        print(f\"Schema match: {schema_match}\")\n",
    "\n",
    "        # Union all years\n",
    "        print(\"Unioning all years...\")\n",
    "        df_all_years_staged = (\n",
    "            df_2024_std\n",
    "            .unionByName(df_2025_std)\n",
    "            .unionByName(df_2023_std)\n",
    "            .unionByName(df_2022_std)\n",
    "        )\n",
    "\n",
    "        # Write to staging (this is the action - no need for count after)\n",
    "        print(\"Writing to staging location...\")\n",
    "        df_all_years_staged.write.mode(\"overwrite\").parquet(\"/opt/spark/resources/Staging_output\")\n",
    "        \n",
    "        print(\"SUCCESS: Staging write completed\")\n",
    "        print(f\"Total records: {count_2022 + count_2023 + count_2024 + count_2025}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014f16b2-ae4b-47ec-b9da-5f46ce98d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
