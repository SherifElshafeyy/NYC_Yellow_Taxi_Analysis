{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b916896-bd4f-4593-87be-fcd8342e3f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"NYC_Taxi_Analysis\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f0226a-8e53-46ae-80cb-b5662844b257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  ***2022 Schema***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb91867-dc1b-4039-98fd-9077d5674490",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39899180-baf8-4608-b929-22bc775ed682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records of 2023 data is : 39656098\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of records of 2023 data is : {df_2022.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b975d8f-f74a-4c4d-977e-5ecab2e80a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d83fb-6d68-4b53-afc8-31949c61b561",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  ***2023 Schema***\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4882ca-acff-40d2-9704-7744903fd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3096945-969b-4136-be73-b7c9f55d1fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records of 2023 data is : 38310226\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of records of 2023 data is : {df_2023.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6445195a-fc7f-4048-9537-20ed31528f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2023.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04633a7f-26f4-403f-88eb-653d2cce5442",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***2024 Schema***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e9889b8-c958-4cf5-bacf-0c72824b9cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5bc8880-80a9-4e45-aa54-c1adddf2f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records of 2024 data is : 41169720\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records of 2024 data is : {df_2024.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f445c1ec-9c56-4714-bb5d-52d9bca62f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2024.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0512e18-d9f1-48e8-a47b-de534d8c3460",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##  ***2025 Schema***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939f84bd-fe6d-4ec0-be04-5bbfa334d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025 = spark.read.parquet(\"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a94b7251-1d8f-4778-a729-b785e0a51747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records of 2025 data is : 40236152\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of records of 2025 data is : {df_2025.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc8bf6ff-4719-414a-822a-1438db19b71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2025.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed0e7b26-764c-4b54-b121-ef43a7b67750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_2022.schema==df_2023.schema == df_2024.schema == df_2025.schema)  \n",
    "\n",
    "#the result is False , so there is a schema mismatch , and to resolve this we should use mergeschema in spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b575eb6f-63ae-4dc6-aa23-f13f2892fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.option(\"mergeSchema\", \"true\").parquet(\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\",\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\",\n",
    "#    \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\"\n",
    "#)\n",
    "\n",
    "#the mergeschema in spark can not work , because it can not merge data types \"Failed to merge incompatible data types \"BIGINT\" and \"INT\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462ba7a-9cd1-405c-823b-d5dfc51b043c",
   "metadata": {},
   "source": [
    "# **Standard Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d1cdc0-d279-4c79-8b21-a2f3022f86b5",
   "metadata": {},
   "source": [
    "### ⚠️ Observed Schema Mismatches\n",
    "\n",
    "- **Data type inconsistencies across years**\n",
    "  - `VendorID`: `long` (2022-2023) vs `integer` (2024–2025)\n",
    "  - `passenger_count`: `double` (2022-2023) vs `long` (2024–2025)\n",
    "  - `RatecodeID`: `double` (2022-2023) vs `long` (2024–2025)\n",
    "  - `PULocationID`, `DOLocationID`: `long` (2022-2023) vs `integer` (2024–2025)\n",
    "\n",
    "- **Column name variation**\n",
    "  - `airport_fee` (2022-2023) vs `Airport_fee` (2024–2025)\n",
    "\n",
    "- **Schema evolution**\n",
    "  - `cbd_congestion_fee` present only in 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d3ed60d-764c-4011-a1f0-68a4c8f6e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, DoubleType, StringType, TimestampType ,TimestampNTZType\n",
    "\n",
    "standard_schema = StructType([\n",
    "    StructField('VendorID', IntegerType(), True),\n",
    "    StructField('tpep_pickup_datetime', TimestampNTZType(), True),\n",
    "    StructField('tpep_dropoff_datetime', TimestampNTZType(), True),\n",
    "    StructField('passenger_count', LongType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', IntegerType(), True),\n",
    "    StructField('DOLocationID', IntegerType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True),\n",
    "    StructField('congestion_surcharge', DoubleType(), True),\n",
    "    StructField('Airport_fee', DoubleType(), True),  # <-- change here\n",
    "    StructField('cbd_congestion_fee', DoubleType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634a6c8-db36-4842-8d43-c5a9b15a5b9e",
   "metadata": {},
   "source": [
    "## Enforce Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69ab8520-b9c7-41dc-9f46-cb3413747c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "def enforce_schema(df, schema):\n",
    "    # Rename existing lower-case column if needed\n",
    "    if \"airport_fee\" in df.columns:\n",
    "        df = df.withColumnRenamed(\"airport_fee\", \"Airport_fee\")\n",
    "    \n",
    "    # Add missing columns and cast\n",
    "    for field in schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df.select([field.name for field in schema.fields])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96801c2c-4dbc-4b2e-a86f-29ce03309d80",
   "metadata": {},
   "source": [
    "### Read 2022 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2c4773c3-0353-4c66-aa06-1a243bc3162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2022\"\n",
    "\n",
    "dfs_2022_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2022-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2022_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5f623eba-1784-4cfe-a8a3-2a85c2003bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2022_std[month] for month in sorted(dfs_2022_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2022_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6cf49bf5-069e-4578-baac-00307e4237ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39656098"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2022_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e6a98-0d08-46c9-8fc7-85facdcf8bbb",
   "metadata": {},
   "source": [
    "### Read 2023 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03b95006-1561-4506-a05d-2160baded09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2023\"\n",
    "\n",
    "dfs_2023_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2023-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2023_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd061bb2-43f2-490e-b656-12f90b77de67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2023_std[month] for month in sorted(dfs_2023_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2023_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e81cb057-0ed8-4826-b296-4651ba9e0cbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38310226"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2023_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be04d12-c956-49ef-8835-f945ebaaffbf",
   "metadata": {},
   "source": [
    "### Read 2024 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "906ac1cb-4fd7-4ac8-b435-65f7c038eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2024\"\n",
    "\n",
    "dfs_2024_std = {}\n",
    "\n",
    "for m in range(1, 13):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2024-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2024_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1f13955-6166-4dc5-a536-ef9df3f3481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2024_std[month] for month in sorted(dfs_2024_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2024_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f8603b87-d84e-4cc9-b865-66685f1582fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41169720"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2024_std.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c4035-243f-4e1e-b640-808e19b9f289",
   "metadata": {},
   "source": [
    "### Read 2025 Parquet Files and Union them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1cfbf8f-79b1-4afa-acce-a18b7244675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/opt/spark/resources/NYC_Yellow_Taxi_Trips/2025\"\n",
    "\n",
    "dfs_2025_std = {}\n",
    "\n",
    "for m in range(1, 11):\n",
    "    month = f\"{m:02d}\"\n",
    "    path = f\"{base_path}/yellow_tripdata_2025-{month}.parquet\"\n",
    "    \n",
    "    df = spark.read.parquet(path)\n",
    "    df_std = enforce_schema(df, standard_schema)\n",
    "    \n",
    "    dfs_2025_std[month] = df_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b645ed93-b67f-4303-88ea-a1508fe9541e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List of all 12 DataFrames\n",
    "dfs_list = [dfs_2025_std[month] for month in sorted(dfs_2025_std.keys())]\n",
    "\n",
    "# Union all months into one DataFrame\n",
    "df_2025_std = reduce(lambda df1, df2: df1.unionByName(df2), dfs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d37ac9a1-aab2-44f1-8385-de1c8dddaf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40236152"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2025_std.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33753981-7b2e-40d8-8d96-cda3a1710afd",
   "metadata": {},
   "source": [
    "# **Union All df Years**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d60b9be8-4b0a-4855-acac-569e9e065cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check schema match\n",
    "print(df_2022_std.schema==df_2023_std.schema==df_2024_std.schema==df_2025_std.schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6392dc1e-b200-4863-bec6-145f513db4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    \n",
    "     df_2024_std\n",
    "    .unionByName(df_2025_std)\n",
    "    .unionByName(df_2023_std)\n",
    "    .unionByName(df_2022_std)\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5ac66e4-55b2-49d6-bdf7-6106c379d519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records of full df is : 159372196\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of records of full df is : {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b752e77-2876-448e-bc67-f7ff9135825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0431a458-2860-4b4c-8ec5-be8af65ee803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|              NULL|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|              NULL|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|              NULL|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|              NULL|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|              NULL|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0|              NULL|\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75|              NULL|\n",
      "|       1| 2024-01-01 00:30:40|  2024-01-01 00:58:40|              0|          3.0|         1|                 N|         246|         231|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0|              NULL|\n",
      "|       2| 2024-01-01 00:26:01|  2024-01-01 00:54:12|              1|         5.44|         1|                 N|         161|         261|           2|       31.0|  1.0|    0.5|       0.0|         0.0|                  1.0|        36.0|                 2.5|        0.0|              NULL|\n",
      "|       2| 2024-01-01 00:28:08|  2024-01-01 00:29:16|              1|         0.04|         1|                 N|         113|         113|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|              NULL|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5da9e4-225f-42fc-8a61-fdf991f4c36c",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ba98a-24b4-4fa2-adf9-bcf71015502d",
   "metadata": {},
   "source": [
    "### **Rename Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "57a28c29-4c9f-4575-91ea-b2d6ed6a9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_col_renamed = (\n",
    "    df\n",
    "    .withColumnRenamed(\"VendorID\", \"Vendor_ID\")\n",
    "    .withColumnRenamed(\"RatecodeID\", \"Ratecode_ID\")\n",
    "    .withColumnRenamed(\"PULocationID\", \"Pickup_Location_ID\")\n",
    "    .withColumnRenamed(\"DOLocationID\", \"Dropoff_Location_ID\")\n",
    "    .withColumnRenamed(\"extra\", \"extra_charges\")\n",
    "    .withColumnRenamed(\"tpep_pickup_datetime\",\"Trip_Pickup_DateTime\")\n",
    "    .withColumnRenamed(\"tpep_dropoff_datetime\",\"Trip_Dropoff_DateTime\")\n",
    "    .withColumn(\n",
    "        'trip_duration_min',\n",
    "        f.round(\n",
    "            (f.unix_timestamp('Trip_Dropoff_DateTime') - f.unix_timestamp('Trip_Pickup_DateTime')) / 60,\n",
    "            2\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "859a5137-2cae-4aa6-b577-37e90a6be40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Vendor_ID: integer (nullable = true)\n",
      " |-- Trip_Pickup_DateTime: timestamp_ntz (nullable = true)\n",
      " |-- Trip_Dropoff_DateTime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- Ratecode_ID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- Pickup_Location_ID: integer (nullable = true)\n",
      " |-- Dropoff_Location_ID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra_charges: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      " |-- trip_duration_min: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_col_renamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4b7f8123-0ee2-44c7-a0d7-b6766602075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-----------------+\n",
      "|Trip_Pickup_DateTime|Trip_Dropoff_DateTime|trip_duration_min|\n",
      "+--------------------+---------------------+-----------------+\n",
      "| 2024-01-01 00:57:55|  2024-01-01 01:17:43|             19.8|\n",
      "| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              6.6|\n",
      "| 2024-01-01 00:17:06|  2024-01-01 00:35:01|            17.92|\n",
      "| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              8.3|\n",
      "| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              6.1|\n",
      "| 2024-01-01 00:54:08|  2024-01-01 01:26:31|            32.38|\n",
      "| 2024-01-01 00:49:44|  2024-01-01 01:15:47|            26.05|\n",
      "| 2024-01-01 00:30:40|  2024-01-01 00:58:40|             28.0|\n",
      "| 2024-01-01 00:26:01|  2024-01-01 00:54:12|            28.18|\n",
      "| 2024-01-01 00:28:08|  2024-01-01 00:29:16|             1.13|\n",
      "| 2024-01-01 00:35:22|  2024-01-01 00:41:41|             6.32|\n",
      "| 2024-01-01 00:25:00|  2024-01-01 00:34:03|             9.05|\n",
      "| 2024-01-01 00:35:16|  2024-01-01 01:11:52|             36.6|\n",
      "| 2024-01-01 00:43:27|  2024-01-01 00:47:11|             3.73|\n",
      "| 2024-01-01 00:51:53|  2024-01-01 00:55:43|             3.83|\n",
      "| 2024-01-01 00:50:09|  2024-01-01 01:03:57|             13.8|\n",
      "| 2024-01-01 00:41:06|  2024-01-01 00:53:42|             12.6|\n",
      "| 2024-01-01 00:52:09|  2024-01-01 00:52:28|             0.32|\n",
      "| 2024-01-01 00:56:38|  2024-01-01 01:03:17|             6.65|\n",
      "| 2024-01-01 00:32:34|  2024-01-01 00:49:33|            16.98|\n",
      "+--------------------+---------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_col_renamed.select('Trip_Pickup_DateTime','Trip_Dropoff_DateTime','trip_duration_min').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fd8ad104-6246-45b6-9d70-8227dae475a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|max(trip_duration_min)|\n",
      "+----------------------+\n",
      "|         1.032205518E7|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_col_renamed.select(f.max(\"trip_duration_min\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad2d32-e66c-4d68-8fbe-d5acd9fd0695",
   "metadata": {},
   "source": [
    "### **Filter bad Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb9fbc-2cd7-443e-90f6-9637f242f147",
   "metadata": {},
   "source": [
    "#### Filter Conditions \n",
    "\n",
    "The following conditions will be used to **filter the dataset**:\n",
    "\n",
    "1. **Passenger count:** Only keep trips where the number of passengers is between 1 and 6 (exclusive of 0 and 7).  \n",
    "\n",
    "2. **Trip distance:** Keep trips where the distance is **zero** (could indicate invalid trips).  \n",
    "\n",
    "3. **Pickup and dropoff location:** Trips where the pickup and dropoff locations are the same.\n",
    "\n",
    "4. **trip_duration**: Duration of Trip should be leass than 2 hours  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6543c81-f91d-4249-86b3-e3ceff10eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_col_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b6ab6b45-2195-432e-a1c1-b329444e6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_filtered = df_col_renamed.filter(\n",
    "    (f.col(\"passenger_count\").between(1, 6)) &\n",
    "    (f.col(\"trip_distance\").between(0,200)) &\n",
    "    (f.col(\"Pickup_Location_ID\") != f.col(\"Dropoff_Location_ID\")) &\n",
    "    (f.col(\"trip_duration_min\").between(0.01,120.1))\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2dff0929-98a3-4d43-8f1c-a99041ab22b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------------+---------------+-------------+-----------+------------------+------------------+-------------------+------------+-----------+-------------+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-----------------+\n",
      "|Vendor_ID|Trip_Pickup_DateTime|Trip_Dropoff_DateTime|passenger_count|trip_distance|Ratecode_ID|store_and_fwd_flag|Pickup_Location_ID|Dropoff_Location_ID|payment_type|fare_amount|extra_charges|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|trip_duration_min|\n",
      "+---------+--------------------+---------------------+---------------+-------------+-----------+------------------+------------------+-------------------+------------+-----------+-------------+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-----------------+\n",
      "+---------+--------------------+---------------------+---------------+-------------+-----------+------------------+------------------+-------------------+------------+-----------+-------------+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " #check applying of filter condition\n",
    "#from pyspark.sql import functions as f\n",
    "\n",
    "#df_filtered.where ( (f.col(\"passenger_count\") == 0) | (f.col(\"trip_distance\")>200) |(f.col(\"trip_duration_min\") >120.1) ).show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2080a1aa-b6bc-471d-a733-c7742fed28be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133027172"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e3ef54dd-bfca-4f69-b495-61bde200d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|max(trip_duration_min)|\n",
      "+----------------------+\n",
      "|                 120.1|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_filtered.select(f.max(\"trip_duration_min\")).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
