version: '3.9'

services:
  # PostgreSQL for Airflow
  postgres:
    image: postgres:15
    container_name: postgres-airflow
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - spark-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Airflow Webserver
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__SECRET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    ports:
      - "8082:8080"
    volumes:
      - ./sandbox/dags:/opt/airflow/dags
      - ./sandbox/plugins:/opt/airflow/plugins
      - ./sandbox/logs:/opt/airflow/logs
      - ./sandbox/notebooks:/opt/airflow/notebooks
      - ./sandbox/spark/app:/opt/spark/app
      - ./sandbox/spark/resources:/opt/spark/resources
    command: webserver
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Airflow Scheduler
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    restart: always
    depends_on:
      postgres:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./sandbox/dags:/opt/airflow/dags
      - ./sandbox/plugins:/opt/airflow/plugins
      - ./sandbox/logs:/opt/airflow/logs
      - ./sandbox/notebooks:/opt/airflow/notebooks
      - ./sandbox/spark/app:/opt/spark/app
      - ./sandbox/spark/resources:/opt/spark/resources
    command: scheduler
    networks:
      - spark-network

  # Airflow Init
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-init
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./sandbox/dags:/opt/airflow/dags
      - ./sandbox/plugins:/opt/airflow/plugins
      - ./sandbox/logs:/opt/airflow/logs
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "Initializing Airflow database..."
        airflow db migrate
        echo "Creating admin user..."
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin || true
        echo "Initialization complete!"
    networks:
      - spark-network

  # Spark Master
  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "18080:8080"
      - "7077:7077"
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
    volumes:
      - ./sandbox/spark/app:/opt/spark/app
      - ./sandbox/spark/resources:/opt/spark/resources
    networks:
      - spark-network

  # Spark Worker
  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    user: root
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=2
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./sandbox/spark/app:/opt/spark/app
      - ./sandbox/spark/resources:/opt/spark/resources
    networks:
      - spark-network

  # Jupyter
  jupyter:
    image: quay.io/jupyter/pyspark-notebook:spark-3.5.1
    container_name: jupyter-lab
    hostname: jupyter-lab
    ports:
      - "8888:8888"
      - "4040:4040"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_LOCAL_IP=jupyter-lab
    volumes:
      - ./sandbox/notebooks:/home/jovyan/work
      - ./sandbox/spark/app:/home/jovyan/app
      - ./sandbox/spark/resources:/opt/spark/resources
    depends_on:
      - spark-master
    networks:
      spark-network:
        aliases:
          - jupyter-lab
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''

volumes:
  postgres_data:

networks:
  spark-network:
    driver: bridge